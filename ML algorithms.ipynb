{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f3c53d5-feef-4b0a-8771-0f4d1343724f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\spark\\\\spark-3.4.2-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d53b1ff0-2936-4ffb-8c7f-3a8cd7342b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d266859-4e7c-4325-a01e-7bddce3427c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21512792-f54e-4556-b7c7-fa692e621fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "\n",
    "sc = SparkSession.builder.appName(\"Product_Recommendation\") \\\n",
    ".config (\"spark.sql.shuffle.partitions\", \"16\") \\\n",
    ".config(\"spark.driver.maxResultSize\",\"4g\") \\\n",
    ".config (\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    ".config(\"spark.driver.memory\", \"4g\") \\\n",
    ".config(\"spark.executor.cores\", \"4\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "sc.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "233809aa-2247-4736-a4a7-f6fc93f6fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sc.read.option('header', True).csv(r\"C:\\Users\\sonal\\OneDrive\\Desktop\\Project\\Group06\\2020-Apr.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1e12912-e4af-4a2e-af8c-2234c2a94234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df = df.withColumn('event_time', to_timestamp('event_time'))\n",
    "df = df.withColumn('date', date_format(\"event_time\", \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b8f30bf-3f14-45c2-81b4-b674ad98a368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+-----+---------+\n",
      "|  user_id|product_id|views|carts|purchases|\n",
      "+---------+----------+-----+-----+---------+\n",
      "|561804169|   1005119| null|  705|     null|\n",
      "|515823933|   4804056|  268|  357|       57|\n",
      "|627964145|   1002544|  386|  305|      186|\n",
      "|636877416|   1004836|    1|  275|        1|\n",
      "|637820403|   1005267|   18|  225|     null|\n",
      "|553146361| 100068493|    1|  224|     null|\n",
      "|553750625|   1004838|   29|  219|        6|\n",
      "|629017322|   1004226|    9|  180|     null|\n",
      "|602700164| 100068493|  181|  178|       88|\n",
      "|515015932|   2702331|  314|  169|      116|\n",
      "|526141229|   1307555|   57|  166|       20|\n",
      "|532696490| 100068488|    3|  164|        1|\n",
      "|518514888|   1005105|  240|  158|       86|\n",
      "|640826351|  12707969|    4|  148|        1|\n",
      "|642438807| 100055405|   11|  144|     null|\n",
      "|621593683|   1002544|   12|  141|        5|\n",
      "|641599967|   1801966|   27|  140|        7|\n",
      "|619714299| 100003951|   22|  138|       10|\n",
      "|563544259|   3601074|   10|  135|        1|\n",
      "|608251121|   1005100|    3|  135|        1|\n",
      "+---------+----------+-----+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the number of interactions between users and products, sorted by carts\n",
    "\n",
    "interactions = df.groupby(['user_id', 'product_id']).agg(sum(when(df['event_type'] == 'view', 1)).alias('views'),\n",
    "                                                         sum(when(df['event_type'] == 'cart', 1)).alias('carts'),\n",
    "                                                         sum(when(df['event_type'] == 'purchase', 1)).alias('purchases'))\n",
    "\n",
    "interactions.sort('carts', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e0459e3-9407-4d4b-89d2-2af2c44401c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    \n",
    "    # Change data types\n",
    "    df = df.withColumn('event_time', to_timestamp('event_time'))\n",
    "    df = df.withColumn('user_id', col('user_id').cast('integer'))\n",
    "    df = df.withColumn('product_id', col('product_id').cast('integer'))\n",
    "    df = df.withColumn('category_id', col('category_id').cast('long'))\n",
    "    \n",
    "    # Limit the number of carts to 1 per session for each user-product pair\n",
    "    cart_df = df.filter(col('event_type') == 'cart')\n",
    "    df = df.filter(col('event_type') != 'cart')\n",
    "    cart_df = cart_df.dropDuplicates(subset=['product_id', 'user_id', 'user_session'])\n",
    "    df = df.union(cart_df)\n",
    "    \n",
    "    # Split category codes into sub categories\n",
    "    '''\n",
    "    from pyspark.sql.functions import split\n",
    "\n",
    "    df = (df\n",
    "      .withColumn('Category', split(df['category_code'], \"\\\\.\").getItem(0))\n",
    "      .withColumn('Sub_category', split(df['category_code'], \"\\\\.\").getItem(1))\n",
    "      .withColumn('Product', split(df['category_code'], \"\\\\.\").getItem(2)))\n",
    "    '''\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9d11aa2-e294-437d-9e19-a15725b0f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4477d2c1-c8b5-4523-816e-3b83f5190f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+----------+\n",
      "|         event_time|event_type|product_id|        category_id|       category_code|  brand|  price|  user_id|        user_session|      date|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+----------+\n",
      "|2020-04-01 05:30:00|      view|   1201465|2232732101407408685|apparel.shoes.sli...|samsung| 230.38|568984877|e2456cef-2d4f-42b...|2020-04-01|\n",
      "|2020-04-01 05:30:01|      view|   1307156|2053013554658804075|electronics.audio...|  apple|1352.67|514955500|38f43134-de83-471...|2020-04-01|\n",
      "+-------------------+----------+----------+-------------------+--------------------+-------+-------+---------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5f33aa5-9ba1-48de-8f22-e36daa2f67e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the timestamp of the most recent event in the df\n",
    "last_date = df.agg(max('event_time')).collect()[0][0]\n",
    "df = df.withColumn('last_date', lit(last_date))\n",
    "\n",
    "# Calculate the recency of each event in terms of days\n",
    "df = df.withColumn('recency', (col('last_date').cast('double') - col('event_time').cast('double')) / 86400)\n",
    "df = df.drop('last_date')\n",
    "\n",
    "# Half-life decay function, the value of an event is halved after 20 days\n",
    "df = df.withColumn('recency_coef', expr('exp(ln(0.5)*recency/20)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86ceb892-086f-4375-bc42-f1b9f913c3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of views, carts and purchases for each user-product pair\n",
    "interactions = df.groupby(['user_id', 'product_id']).agg(sum(when(df['event_type'] == 'view', 1) * df['recency_coef']).alias('views'),\n",
    "                                                         sum(when(df['event_type'] == 'cart', 1) * df['recency_coef']).alias('carts'),\n",
    "                                                         sum(when(df['event_type'] == 'purchase', 1) * df['recency_coef']).alias('purchases'))\n",
    "interactions = interactions.na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f5d1b5d-f12b-4a55-8674-4640a21285cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_interaction_matrix(df, view_weight=0.1, cart_weight=0.3, purchase_weight=1.0):\n",
    " \n",
    "    # Create a new column with the weighted interaction value\n",
    "    df = df.withColumn('interaction', view_weight * col('views') + cart_weight * col('carts') + purchase_weight * col('purchases'))\n",
    "    \n",
    "    # Use log10 value for views, carts and purchases\n",
    "    df = df.withColumn('interaction', log10(col('interaction') + 1))\n",
    "    \n",
    "    # Set the max possible value to 100 (log100 = 2)\n",
    "    df = df.withColumn('interaction', when(col('interaction') > 2, 2).otherwise(col('interaction')))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01c1150b-2639-4994-98f0-96b34249e857",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_matrix = calculate_interaction_matrix(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb334c0d-e779-4890-9183-1c3f79353d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------------+------------------+------------------+------------------+\n",
      "|  user_id|product_id|             views|             carts|         purchases|       interaction|\n",
      "+---------+----------+------------------+------------------+------------------+------------------+\n",
      "|582249321|  17303060| 151.2180522911544|  70.0345391662787| 64.82335487004885|               2.0|\n",
      "|522016074|   1004246| 91.25545831347586| 50.85077913559578| 80.96545496214094|               2.0|\n",
      "|515015932|   2702331| 229.7294101275016|46.970533624974735| 84.96658287445932|               2.0|\n",
      "|627964145|   1002544|231.81214397099683|  85.2906523862065|111.82412526370732|               2.0|\n",
      "|637890453|   1004625| 982.2671413260865|               0.0|               0.0|1.9966286100423212|\n",
      "|638901021| 100068488| 129.7541250342781|49.259348232049256| 61.74705040774978|1.9566498623193596|\n",
      "|518514888|   1005105|153.96318975415417|57.433089339174025|55.487773065298995| 1.949946029817058|\n",
      "|648484033|   1801690| 91.25963303343481| 36.25809463408524| 67.30184276579874|1.9459864479653475|\n",
      "|602700164| 100068493|106.81131571581626| 51.55388495503125| 51.31879664452197|1.8946820325553573|\n",
      "|550985043|   1801503| 773.8497532956708|               0.0|               0.0|1.8942328259907733|\n",
      "|629426279|   1004226|116.00778227627676| 37.02106212045384| 53.45697013508085| 1.887415109441116|\n",
      "|637084523|   1004249| 101.9486295672884|51.958718748325765|47.106519022925546| 1.868579774855716|\n",
      "|566213018|   1005121| 80.98754134428178| 46.21719199872897| 48.16370539415844|1.8520382594864622|\n",
      "|548811292|   1004958| 78.73948146934598| 43.69117727520251| 47.00455737410116|1.8387600745572759|\n",
      "|515823933|   4804056|183.06195315667745| 28.33238645222361| 38.28977299809272| 1.820173102937237|\n",
      "|515145806| 100189460| 87.57105283484329| 36.37094401941229| 43.03950729611169|1.8041932609622393|\n",
      "|548782814|   1005115| 81.42464407639804| 20.33734246068748| 48.29364260752187| 1.803028822326165|\n",
      "|638235902|   1005121| 83.11047333662762|42.837550459661706|41.129637724126866| 1.801348477665024|\n",
      "|603865034|   1005160|56.190239799578094|  40.9638267265828| 42.79709423019465| 1.790322230422504|\n",
      "|635021608|   1005132| 61.88907176535972| 44.86951784782185| 40.66428496530891|1.7875599859566518|\n",
      "+---------+----------+------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  User-product pairs with the highest interaction scores\n",
    "\n",
    "interaction_matrix.sort('interaction', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7249a14a-b232-40ca-a001-aadab2a88d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_matrix_1K = interaction_matrix.limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94b38205-c004-4bfd-9fd7-bfae64c370fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------------------+------------------+------------------+--------------------+\n",
      "|  user_id|product_id|              views|             carts|         purchases|         interaction|\n",
      "+---------+----------+-------------------+------------------+------------------+--------------------+\n",
      "|568984877|   1201465|0.35355353241305676|               0.0|               0.0|0.015089433333691357|\n",
      "|633281427| 100077607|  5.625789535792961|0.4788379411010751|0.4788446637645328|  0.3394663481190854|\n",
      "|548207294|   1005105| 2.5146706936722643|               0.0|               0.0| 0.09741942623328653|\n",
      "+---------+----------+-------------------+------------------+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interaction_matrix_1K.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c91940c4-1ff1-4890-9119-429593ae2296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "def simple_als(interaction_matrix):\n",
    "    \n",
    "    # Train-test split\n",
    "    (train, test) = interaction_matrix.randomSplit([0.8, 0.2])\n",
    "    \n",
    "    # Initialize the model with the optimized parameters\n",
    "    als = ALS(userCol='user_id', itemCol='product_id', ratingCol='interaction', \n",
    "              alpha=1, regParam=0.005, rank=15, implicitPrefs=True, \n",
    "              nonnegative=True, coldStartStrategy='drop')\n",
    "\n",
    "    # Fit the ALS model on the ratings data\n",
    "    model = als.fit(train)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.transform(test)\n",
    "    # Calculate the RMSE and MAE metrics\n",
    "    evaluator = RegressionEvaluator(metricName='rmse', labelCol='interaction', predictionCol='prediction')\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    mae = evaluator.setMetricName('mae').evaluate(predictions)\n",
    "    print('test rmse:' + str(rmse) + ' mae:' + str(mae))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5323130c-4cbe-4f87-ae59-a1f90e7d81ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test rmse:0.08876871352724139 mae:0.06204417585520686\n"
     ]
    }
   ],
   "source": [
    "als_model = simple_als(interaction_matrix_1K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9c31050-0fb5-42ff-967e-f18b30ab19cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 random users\n",
    "user_subset = [568984877, 633281427, 548207294]\n",
    "\n",
    "# Recommend top 2 products for the users\n",
    "recommendations = sc.createDataFrame([(user, 0) for user in user_subset], ['user_id', 'product_id'])\n",
    "recommendations = als_model.recommendForUserSubset(recommendations, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "48a57e79-1d08-4119-bbba-b44d42593794",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1086.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 654.0 failed 1 times, most recent failure: Lost task 1.0 in stage 654.0 (TID 1885) (localhost executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:601)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:583)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:772)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:757)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:601)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:583)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:772)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:757)\r\n\t... 22 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m recommendations\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical))\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1086.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 654.0 failed 1 times, most recent failure: Lost task 1.0 in stage 654.0 (TID 1885) (localhost executor driver): org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:601)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:583)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:772)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:757)\r\n\t... 22 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:601)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:583)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:772)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:757)\r\n\t... 22 more\r\n"
     ]
    }
   ],
   "source": [
    "recommendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0016beb-6736-4d5b-99d1-c37e4028262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select user 564068124\n",
    "\n",
    "recs_for_user_1 = sc.createDataFrame(recommendations.collect()[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f3d06e-bb93-4f0a-8c3c-7341e9776c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Products with the highest number of interactions for the user\n",
    "\n",
    "interactions.filter(col('user_id') == 564068124).sort('purchases', ascending=False).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
